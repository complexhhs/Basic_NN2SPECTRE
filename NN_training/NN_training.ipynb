{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters\n",
    "    Neural network \n",
    "        # of hidden layers\n",
    "        # of hidden layer nodes\n",
    "    Training\n",
    "        epochs\n",
    "        batch_size\n",
    "        learning_rate\n",
    "        scheduler(StepLR)\n",
    "            gamma\n",
    "            step_size\n",
    "''' \n",
    "no_hidden_layers = 5\n",
    "no_hidden_nodes = 64\n",
    "epochs = 2000\n",
    "batch_size=128\n",
    "learning_rate = 1e-03\n",
    "gamma=0.97\n",
    "step_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minmax_extraction_current(x,y,eps=1e-12,is_nmos=True):\n",
    "    '''\n",
    "    For train data preprocessing\n",
    "    Caution: Extract this variables, only train data!\n",
    "    '''\n",
    "    global vds_min, vds_max, vgs_min, vgs_max, vbs_min, vbs_max, Id_min, Id_max\n",
    "    x = np.array(x).ravel()\n",
    "    y = np.array(y).ravel()\n",
    "    vds_min, vds_max = x[:,0].min(), x[:,0].max()\n",
    "    vgs_min, vgs_max = x[:,1].min(), x[:,1].max()\n",
    "    vbs_min, vbs_max = x[:,2].min(), x[:,2].max()\n",
    "    if is_nmos is False:\n",
    "        y *= -1\n",
    "    y = np.where(y<0, 0, y) + eps\n",
    "    y_dum = np.log10(y) # min value == np.log10(eps)\n",
    "    assert y_dum.min() == np.log10(eps), \"Check your data or preprocess equation\"\n",
    "    Id_min, Id_max = y_dum.min(), y_dum.max()\n",
    "\n",
    "def train_minmax_extraction_charge(x,y,eps=1e-12,is_nmos=True):\n",
    "    '''\n",
    "    For train data preprocessing\n",
    "    Caution: Extract this variables, only train data!\n",
    "    '''\n",
    "    global vds_min, vds_max, vgs_min, vgs_max, vbs_min, vbs_max, Qd_min, Qd_max, Qg_min, Qg_max, Qb_min, Qb_max\n",
    "    x = np.array(x).ravel()\n",
    "    y = np.array(y).ravel()\n",
    "    vds_min, vds_max = x[:,0].min(), x[:,0].max()\n",
    "    vgs_min, vgs_max = x[:,1].min(), x[:,1].max()\n",
    "    vbs_min, vbs_max = x[:,2].min(), x[:,2].max()\n",
    "    Qd_min, Qd_max = y[:,0].min(), y[:,0].max()\n",
    "    Qg_min, Qg_max = y[:,1].min(), y[:,1].max()\n",
    "    Qb_min, Qb_max = y[:,2].min(), y[:,2].max()\n",
    "    \n",
    "def MyPreprocess_I(x,y, eps=1e-12, is_nmos=True):\n",
    "    '''\n",
    "    x -> Vds, Vgs Vbs\n",
    "    y -> Ids\n",
    "    Preprocess equation\n",
    "        X\n",
    "            Vds' = (x[:,0] - vds_min)/(vds_max - vds_min)\n",
    "            Vgs' = (x[:,1] - vgs_min)/(vgs_max - vgs_min)\n",
    "            Vbs' = (x[:,2] - vbs_min)/(vbs_max - vbs_min)\n",
    "            prec_x = [Vds', Vgs', Vbs']\n",
    "        Y\n",
    "            y = cleansing y \n",
    "            y_dum = np.log10(y)\n",
    "            prec_y = (y_dum - Id_min)/(Id_max - Id_min)\n",
    "            ! cleansing: \n",
    "                - y < 0 data elimination \n",
    "                - y += eps\n",
    "    is_nmos:\n",
    "        default: True(NMOS transistor), False(PMOS transistor)\n",
    "            -> In calse of False, y *= -1\n",
    "    '''\n",
    "    x,y = np.array(x).ravel(), np.array(y).ravel()\n",
    "    x[:,0] = (x[:,0] - vds_min) / (vds_max - vds_min)\n",
    "    x[:,1] = (x[:,1] - vgs_min) / (vgs_max - vgs_min)\n",
    "    x[:,2] = (x[:,2] - vbs_min) / (vbs_max - vbs_min)\n",
    "    prec_x = x\n",
    "\n",
    "    if is_nmos is False:\n",
    "        y *= -1\n",
    "    y = np.where(y<0, 0, y) + eps\n",
    "    y_dum = np.log10(y) # min value == np.log10(eps)\n",
    "    prec_y = (y_dum - Id_min)/(Id_max - Id_min)\n",
    "    return prec_x, prec_y\n",
    "\n",
    "def MyPostprocess_I(y,is_nmos=True):\n",
    "    '''\n",
    "    Reversal of preprocess procedure\n",
    "    y_dum = y*(Id_max - Id_min) + Id_min\n",
    "    post_y = 10**y_dum\n",
    "\n",
    "    '''\n",
    "    y_dum = y*(Id_max - Id_min) + Id_min\n",
    "    post_y = 10**y_dum\n",
    "    if is_nmos is False:\n",
    "        post_y *= -1\n",
    "    return post_y\n",
    "\n",
    "def MyPreprocess_Q(x,y, eps=1e-12, is_nmos=True):\n",
    "    '''\n",
    "    x -> Vds, Vgs Vbs\n",
    "    y -> Qd, Qg, Qb\n",
    "    Preprocess equation\n",
    "        X\n",
    "            Vds' = (x[:,0] - vds_min)/(vds_max - vds_min)\n",
    "            Vgs' = (x[:,1] - vgs_min)/(vgs_max - vgs_min)\n",
    "            Vbs' = (x[:,2] - vbs_min)/(vbs_max - vbs_min)\n",
    "            prec_x = [Vds', Vgs', Vbs']\n",
    "        Y\n",
    "            prec_y = (y_dum - Id_min)/(Id_max - Id_min)\n",
    "    '''\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    '''\n",
    "    Custom dataset -> simple MOSFET data preprocessing \n",
    "    '''\n",
    "    def __init__(self,data_numpy,is_nmos=True,is_current=True):\n",
    "        self.data_numpy = data_numpy\n",
    "        self.is_nmos = is_nmos\n",
    "        self.is_current = is_current\n",
    "        self.x = self.data_numpy[:,:3]\n",
    "        self.y = self.data_numpy[:,4]\n",
    "        if self.is_current:\n",
    "            Mypreprocess = Mypreprocess_I\n",
    "        else:\n",
    "            Mypreprocess = Mypreprocess_Q\n",
    "        self.x, self.y = Mypreprocess(self.x, self.y, self.is_nmos)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.DoubleTensor(self.x[idx,:]).to(device), torch.DoubleTensor(self.y[idx,:]).view(-1,1).to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.x[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_data, Valid_data, Test_data\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(train_data)\n",
    "valid_dataset = MyDataset(valid_data)\n",
    "test_dataset = MyDataset(test_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_FC(nn.Module):\n",
    "    def __init__(self,input_nodes=3,output_nodes=1,num_hidden_layer=5,num_hidden_nodes=64):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        # input_layer\n",
    "        self.fc_layers.append(nn.Linear(self.input_nodes, self.num_hidden_nodes))\n",
    "        # hidden_layer\n",
    "        for _ in range(self.num_hidden_layer):\n",
    "            self.fc_layers.append(nn.Linear(self.num_hidden_nodes, self.num_hidden_nodes))\n",
    "        # output_layer\n",
    "        self.output_layer = nn.Linear(self.num_hidden_nodes,self.output_nodes)\n",
    "        self.elu = F.elu()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc_layers:\n",
    "            x = layer(x)\n",
    "            x = self.elu(x)\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimization, scheduler definition\n",
    "myModel_I = Simple_FC(input_nodes=3, output_nodes=1, num_hidden_layer=no_hidden_layers, num_hidden_nodes=no_hidden_nodes)\n",
    "optimizer = optim.Adam(myModel_I.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training - I model\n",
    "best_loss = np.inf\n",
    "cnt = 0\n",
    "for epoch in range(epochs):\n",
    "    cnt += 1\n",
    "    train_loss = 0\n",
    "    for x, y in train_dataloader:\n",
    "        y_pred = myModel_I(x)\n",
    "        loss = torch.mean((y-y_pred)**2)\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_dataloader:\n",
    "            y_pred = myModel_I(x)\n",
    "            loss = torch.mean((y-y_pred)**2)\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        best_model = copy.deepcopy(myModel)\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epochs // cnt == 10:\n",
    "        cnt = 0\n",
    "        print(f'Training log - Progress: {(epoch+1)/epochs*100}%')\n",
    "        print(f'Training loss: {train_loss}, Validation loss: {valid_loss}, Best validation loss: {best_loss} at {best_epoch}.')\n",
    "\n",
    "# Test data Quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model save - jit trace method use!\n",
    "torch.jit.save(best_model,'./BSS_Imodel_NN.pt')\n",
    "\n",
    "# I model - data parameter save\n",
    "params = {\n",
    "    \"VDS\": {\n",
    "        \"min\": vds_min,\n",
    "        \"max\": vds_max\n",
    "    },\n",
    "    \"VGS\":{\n",
    "        \"min\": vgs_min,\n",
    "        \"max\": vgs_max\n",
    "    },\n",
    "    \"VBS\":{\n",
    "        \"min\": vbs_min,\n",
    "        \"max\": vbs_max\n",
    "    },\n",
    "    \"Id\": {\n",
    "        \"min\": Id_min,\n",
    "        \"max\": Id_max\n",
    "    },\n",
    "}\n",
    "with open('./BSS_Imodel_params.json','w') as json_file:\n",
    "    json.dump(params,json_file,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimization, scheduler definition\n",
    "myModel_Q = Simple_FC(input_nodes=3, output_nodes=3, num_hidden_layer=no_hidden_layers, num_hidden_nodes=no_hidden_nodes)\n",
    "optimizer = optim.Adam(myModel_Q.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training - Q model\n",
    "best_loss = np.inf\n",
    "cnt = 0\n",
    "for epoch in range(epochs):\n",
    "    cnt += 1\n",
    "    train_loss = 0\n",
    "    for x, y in train_dataloader:\n",
    "        y_pred = myModel_Q(x)\n",
    "        loss = torch.mean((y-y_pred)**2)\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_dataloader:\n",
    "            y_pred = myModel_Q(x)\n",
    "            loss = torch.mean((y-y_pred)**2)\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        best_model = copy.deepcopy(myModel)\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epochs // cnt == 10:\n",
    "        cnt = 0\n",
    "        print(f'Training log - Progress: {(epoch+1)/epochs*100}%')\n",
    "        print(f'Training loss: {train_loss}, Validation loss: {valid_loss}, Best validation loss: {best_loss} at {best_epoch}.')\n",
    "\n",
    "# Test data Quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save - jit trace method use!\n",
    "torch.jit.save(best_model,'./BSS_Qmodel_NN.pt')\n",
    "\n",
    "# Q model - data parameter save\n",
    "params = {\n",
    "    \"VDS\": {\n",
    "        \"min\": vds_min,\n",
    "        \"max\": vds_max\n",
    "    },\n",
    "    \"VGS\":{\n",
    "        \"min\": vgs_min,\n",
    "        \"max\": vgs_max\n",
    "    },\n",
    "    \"VBS\":{\n",
    "        \"min\": vbs_min,\n",
    "        \"max\": vbs_max\n",
    "    },\n",
    "    \"Qd\": {\n",
    "        \"min\": Qd_min,\n",
    "        \"max\": Qd_max\n",
    "    },\n",
    "    \"Qg\": {\n",
    "        \"min\": Qg_min,\n",
    "        \"max\": Qg_max\n",
    "    },\n",
    "    \"Qb\": {\n",
    "        \"min\": Qb_min,\n",
    "        \"max\": Qb_max\n",
    "    },\n",
    "}\n",
    "with open('./BSS_Qmodel_params.json','w') as json_file:\n",
    "    json.dump(params,json_file,indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_scratch",
   "language": "python",
   "name": "rl_scratch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
